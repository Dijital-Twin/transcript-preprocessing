{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/volthai7us/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gensim\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import json\n",
    "import nltk\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_split(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "def preprocess_speaker(speaker): \n",
    "    file_path = f'./data/transcripts/friends-1-227-{speaker}-pair.json'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        \n",
    "    all_answers = [item['answer'] for item in data]\n",
    "\n",
    "    all_answers = [clean_and_split(answer) for answer in all_answers]\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    \n",
    "    def remove_stopwords(answer):\n",
    "        filtered_words = [word for word in answer if word.lower() not in stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    all_answers = [remove_stopwords(answer) for answer in all_answers]\n",
    "\n",
    "    return all_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = (\"Rachel\", \"Chandler\", \"Phoebe\", \"Monica\", \"Ross\", \"Joey\")\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for index, speaker in enumerate(speakers):\n",
    "    all_answers = preprocess_speaker(speaker=speaker)\n",
    "    speaker_data = [(answer, index) for answer in all_answers]\n",
    "    sample_size = min(5000, len(speaker_data))\n",
    "    random_sample = random.sample(speaker_data, sample_size)\n",
    "    all_data.extend(random_sample)\n",
    "    \n",
    "speaker_combinations = []\n",
    "\n",
    "for speaker1 in speakers:\n",
    "    for speaker2 in speakers:\n",
    "        combination = f\"{speaker1}-{speaker2}\"\n",
    "        speaker_combinations.append(combination)\n",
    "\n",
    "df = pd.DataFrame(all_data, columns=['text', 'speaker'])\n",
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rachel_df = df[df['speaker'] == 0]\n",
    "other_df = df[df['speaker'] != 0]\n",
    "other_df = other_df.sample(frac=1)\n",
    "other_df = other_df[:rachel_df.shape[0]]\n",
    "other_df['speaker'] = 1\n",
    "df = pd.concat([rachel_df, other_df])\n",
    "df = df.sample(frac=1)\n",
    "speakers = [\"Rachel\", \"Other\"]\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['text'] = df['text'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            #if len(word) < 0:\n",
    "            if len(word) <= 0:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.00001 , random_state=42)\n",
    "\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r['speaker']]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), tags=[r['speaker']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_fatures = 500000\n",
    "MAX_SEQUENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9094 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "print('Found %s unique tokens.' % len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (9094, 30)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([TaggedDocument(words=['brooklyn'], tags=[0]),\n",
       "       TaggedDocument(words=['fine', 'right', 'let', 'ah', 'let', 'take', 'break', 'let', 'cool', 'okay', 'let', 'get', 'frozen', 'yoghurt', 'something'], tags=[1]),\n",
       "       TaggedDocument(words=['really', 'think', 'say', 'goodbye', 'care'], tags=[0]),\n",
       "       ..., TaggedDocument(words=['tired'], tags=[1]),\n",
       "       TaggedDocument(words=['wow', 'tough', 'one', 'think', 'gon', 'na', 'go', 'dog'], tags=[0]),\n",
       "       TaggedDocument(words=['lookin', 'upside', 'know', 'matter'], tags=[0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tagged.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9093/9093 [00:00<00:00, 4450788.46it/s]\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(dm=1, dm_mean=1, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\n",
    "d2v_model.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9093/9093 [00:00<00:00, 6042269.69it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6480680.76it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6134599.69it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6613283.56it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7065358.70it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7119433.69it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6025087.88it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 3288678.65it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7098233.07it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7189218.90it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7114121.67it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7099554.41it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6972359.46it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7021135.18it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7171644.65it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7159528.12it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7069287.54it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6995378.99it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7148792.18it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7043177.52it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7065358.70it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6729981.70it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7004372.13it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6781437.82it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 6786264.46it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7098233.07it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7182449.39it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7075845.32it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7187863.98it/s]\n",
      "100%|██████████| 9093/9093 [00:00<00:00, 7060127.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.8 s, sys: 56.4 ms, total: 3.86 s\n",
      "Wall time: 3.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    d2v_model.alpha -= 0.002\n",
    "    d2v_model.min_alpha = d2v_model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(d2v_model.wv.index_to_key)+ 1, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, vec in enumerate(d2v_model.dv.vectors):\n",
    "    while i in vec <= 1000:\n",
    "          embedding_matrix[i]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tim', 0.5800897479057312),\n",
       " ('sous', 0.5617893934249878),\n",
       " ('goodie', 0.551600992679596),\n",
       " ('buffay', 0.5263261198997498),\n",
       " ('steer', 0.5143299698829651),\n",
       " ('intentions', 0.5115000605583191),\n",
       " ('phaybobo', 0.49510785937309265),\n",
       " ('quantity', 0.4938683807849884),\n",
       " ('sock', 0.4937157928943634),\n",
       " ('ann', 0.4837372899055481)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.wv.most_similar(positive=['phoebe'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 30, 30)            164820    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 30)                7320      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                372       \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 172538 (673.98 KB)\n",
      "Trainable params: 172538 (673.98 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "# init layer\n",
    "model = Sequential()\n",
    "\n",
    "# emmbed word vectors\n",
    "model.add(Embedding(len(d2v_model.wv.index_to_key)+1,30,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n",
    "\n",
    "# learn the correlations\n",
    "def split_input(sequence):\n",
    "     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\n",
    "model.add(LSTM(30,return_sequences=False))\n",
    "model.add(Dense(12,activation=\"softmax\"))\n",
    "model.add(Dense(2,activation=\"softmax\"))\n",
    "\n",
    "# output model skeleton\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7729, 30) (7729, 2)\n",
      "(1365, 30) (1365, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['speaker']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "242/242 - 1s - loss: 0.3783 - acc: 0.8586 - 1s/epoch - 6ms/step\n",
      "Epoch 2/50\n",
      "242/242 - 1s - loss: 0.3654 - acc: 0.8660 - 1s/epoch - 5ms/step\n",
      "Epoch 3/50\n",
      "242/242 - 1s - loss: 0.3586 - acc: 0.8675 - 1s/epoch - 5ms/step\n",
      "Epoch 4/50\n",
      "242/242 - 1s - loss: 0.3532 - acc: 0.8702 - 1s/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "242/242 - 1s - loss: 0.3431 - acc: 0.8753 - 1s/epoch - 5ms/step\n",
      "Epoch 6/50\n",
      "242/242 - 1s - loss: 0.3382 - acc: 0.8749 - 1s/epoch - 5ms/step\n",
      "Epoch 7/50\n",
      "242/242 - 1s - loss: 0.3301 - acc: 0.8793 - 1s/epoch - 5ms/step\n",
      "Epoch 8/50\n",
      "242/242 - 1s - loss: 0.3256 - acc: 0.8794 - 1s/epoch - 5ms/step\n",
      "Epoch 9/50\n",
      "242/242 - 1s - loss: 0.3177 - acc: 0.8828 - 1s/epoch - 5ms/step\n",
      "Epoch 10/50\n",
      "242/242 - 1s - loss: 0.3142 - acc: 0.8814 - 1s/epoch - 5ms/step\n",
      "Epoch 11/50\n",
      "242/242 - 1s - loss: 0.3120 - acc: 0.8837 - 1s/epoch - 5ms/step\n",
      "Epoch 12/50\n",
      "242/242 - 1s - loss: 0.3066 - acc: 0.8863 - 1s/epoch - 5ms/step\n",
      "Epoch 13/50\n",
      "242/242 - 1s - loss: 0.2980 - acc: 0.8898 - 1s/epoch - 5ms/step\n",
      "Epoch 14/50\n",
      "242/242 - 1s - loss: 0.2924 - acc: 0.8913 - 1s/epoch - 5ms/step\n",
      "Epoch 15/50\n",
      "242/242 - 1s - loss: 0.2865 - acc: 0.8917 - 1s/epoch - 5ms/step\n",
      "Epoch 16/50\n",
      "242/242 - 1s - loss: 0.2804 - acc: 0.8933 - 1s/epoch - 5ms/step\n",
      "Epoch 17/50\n",
      "242/242 - 1s - loss: 0.2786 - acc: 0.8931 - 1s/epoch - 5ms/step\n",
      "Epoch 18/50\n",
      "242/242 - 1s - loss: 0.2687 - acc: 0.8958 - 1s/epoch - 5ms/step\n",
      "Epoch 19/50\n",
      "242/242 - 1s - loss: 0.2607 - acc: 0.8991 - 1s/epoch - 6ms/step\n",
      "Epoch 20/50\n",
      "242/242 - 1s - loss: 0.2542 - acc: 0.9008 - 1s/epoch - 6ms/step\n",
      "Epoch 21/50\n",
      "242/242 - 1s - loss: 0.2505 - acc: 0.9021 - 1s/epoch - 6ms/step\n",
      "Epoch 22/50\n",
      "242/242 - 1s - loss: 0.2447 - acc: 0.9034 - 1s/epoch - 6ms/step\n",
      "Epoch 23/50\n",
      "242/242 - 1s - loss: 0.2453 - acc: 0.9021 - 1s/epoch - 6ms/step\n",
      "Epoch 24/50\n",
      "242/242 - 1s - loss: 0.2370 - acc: 0.9026 - 1s/epoch - 6ms/step\n",
      "Epoch 25/50\n",
      "242/242 - 1s - loss: 0.2329 - acc: 0.9065 - 1s/epoch - 5ms/step\n",
      "Epoch 26/50\n",
      "242/242 - 1s - loss: 0.2286 - acc: 0.9105 - 1s/epoch - 5ms/step\n",
      "Epoch 27/50\n",
      "242/242 - 1s - loss: 0.2243 - acc: 0.9089 - 1s/epoch - 5ms/step\n",
      "Epoch 28/50\n",
      "242/242 - 1s - loss: 0.2294 - acc: 0.9062 - 1s/epoch - 5ms/step\n",
      "Epoch 29/50\n",
      "242/242 - 1s - loss: 0.2262 - acc: 0.9089 - 1s/epoch - 5ms/step\n",
      "Epoch 30/50\n",
      "242/242 - 1s - loss: 0.2237 - acc: 0.9102 - 1s/epoch - 5ms/step\n",
      "Epoch 31/50\n",
      "242/242 - 1s - loss: 0.2146 - acc: 0.9114 - 1s/epoch - 5ms/step\n",
      "Epoch 32/50\n",
      "242/242 - 1s - loss: 0.2110 - acc: 0.9164 - 1s/epoch - 5ms/step\n",
      "Epoch 33/50\n",
      "242/242 - 1s - loss: 0.2076 - acc: 0.9176 - 1s/epoch - 5ms/step\n",
      "Epoch 34/50\n",
      "242/242 - 1s - loss: 0.2069 - acc: 0.9164 - 1s/epoch - 6ms/step\n",
      "Epoch 35/50\n",
      "242/242 - 1s - loss: 0.2049 - acc: 0.9175 - 1s/epoch - 5ms/step\n",
      "Epoch 36/50\n",
      "242/242 - 1s - loss: 0.2058 - acc: 0.9167 - 1s/epoch - 5ms/step\n",
      "Epoch 37/50\n",
      "242/242 - 1s - loss: 0.2022 - acc: 0.9173 - 1s/epoch - 5ms/step\n",
      "Epoch 38/50\n",
      "242/242 - 1s - loss: 0.2002 - acc: 0.9191 - 1s/epoch - 5ms/step\n",
      "Epoch 39/50\n",
      "242/242 - 1s - loss: 0.2013 - acc: 0.9194 - 1s/epoch - 5ms/step\n",
      "Epoch 40/50\n",
      "242/242 - 1s - loss: 0.2037 - acc: 0.9169 - 1s/epoch - 5ms/step\n",
      "Epoch 41/50\n",
      "242/242 - 1s - loss: 0.1963 - acc: 0.9212 - 1s/epoch - 5ms/step\n",
      "Epoch 42/50\n",
      "242/242 - 1s - loss: 0.1935 - acc: 0.9237 - 1s/epoch - 6ms/step\n",
      "Epoch 43/50\n",
      "242/242 - 1s - loss: 0.1920 - acc: 0.9225 - 1s/epoch - 5ms/step\n",
      "Epoch 44/50\n",
      "242/242 - 1s - loss: 0.1929 - acc: 0.9212 - 1s/epoch - 5ms/step\n",
      "Epoch 45/50\n",
      "242/242 - 1s - loss: 0.1922 - acc: 0.9222 - 1s/epoch - 5ms/step\n",
      "Epoch 46/50\n",
      "242/242 - 1s - loss: 0.1890 - acc: 0.9224 - 1s/epoch - 6ms/step\n",
      "Epoch 47/50\n",
      "242/242 - 1s - loss: 0.1853 - acc: 0.9257 - 1s/epoch - 5ms/step\n",
      "Epoch 48/50\n",
      "242/242 - 1s - loss: 0.1867 - acc: 0.9242 - 1s/epoch - 5ms/step\n",
      "Epoch 49/50\n",
      "242/242 - 1s - loss: 0.1875 - acc: 0.9226 - 1s/epoch - 5ms/step\n",
      "Epoch 50/50\n",
      "242/242 - 1s - loss: 0.1895 - acc: 0.9234 - 1s/epoch - 5ms/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "history=model.fit(X_train, Y_train, epochs=50, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 - 0s - loss: 0.1849 - acc: 0.9238 - 296ms/epoch - 1ms/step\n",
      "43/43 - 0s - loss: 1.5099 - acc: 0.5451 - 61ms/epoch - 1ms/step\n",
      "Train: 0.924, Test: 0.5451\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n",
    "_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print('Train: %.3f, Test: %.4f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
